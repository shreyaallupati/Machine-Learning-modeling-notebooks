# -*- coding: utf-8 -*-
"""Customer chrun project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1drRa1g61dHmnGhWhd8MOUkPAfKNBb1Zw

# Project Title :Customer Churn Analytics:

# Abstract

In the competitive landscape of telecommunications, customer churn poses a significant challenge, impacting revenue and market share. This project aims to analyze customer churn patterns within a telecom company to understand the underlying factors contributing to customer attrition. The study leverages historical customer data, including demographic information, service usage metrics, and customer interaction logs. Through exploratory data analysis and predictive modeling techniques, key drivers of churn will be identified, such as service dissatisfaction, pricing strategies, contract terms, and customer support effectiveness.

# Problem Statement:

The objective of this project is to analyze customer churn in a telecom company. Customer churn refers to the phenomenon where customers switch from one service provider to another or cancel their subscription altogether. By analyzing customer chum patterns, we aim to identify the factors that contribute to churn and develop strategies to mitigate it.

# Project Description:

In this project, we will work with a dataset from a telecom company that includes information about their customers, such as demographics, customer Accounting information, Service information. The dataset will also include a churn indicator that specifies nether a customer has churned or not.

Desired problen
come(Objective or goal)The main objective is to find out the reasons for call drops and voice connectivity
Built a classification predictive model to predict call drop

# DesiredOutcome:

our main goal is to  bulid a computer program that can predict when a customer might leave the company

# Algorithms:

LogisticRegression,DecisionTreeClassifier,RandomForestClassifer,AddaboostClassifier,GradientBoostClassifer

# About Data

Data is divided into 3 Types

# Demographic information:

• gender: Whether the customer is a male or a female.

• SeniorCitizen: Whether the customer is a senior citizen or not (1, 0).

• Partner: Whether the customer has a partner or not (Yes, No)

• Dependents : Whether the customer has dependents or not (Yes, No)

# Customer Acconting Information:

• Contract: The contract term of the customer (Month-to-month, One year, Two year)

• PaperlessBilling : Whether the customer has paperless billing or not (Yes, No)

• MonthlyCharges: The amount charged to the customer monthly

• TotalCharges: The total amount charged to the customer

• tenure: Number of months the customer has stayed with the company

• PaymentMethod: The customer's payment method (Electronic check, Mailed check, Bank transfer (au card (automatic))

• customeriD: Customer ID

# Service information

PhoneService: Whether the customer has a phone service or not (yes, No)

• MultipleLines: Whether the customer has multiple lines or not (yes, No, No phone service)

• InternetService: Customer's internet service provider (DSL, Fiber optic, No)

• OnlineSecurity: Whether the customer has online security or not (yes, No, No internet service)

• OnlineBackup: Whether the customer has online backup or not (Yes, No, No internet service)

• DeviceProtection: Whether the customer has device protection or not (yes, No, No internet service)

• TechSupport: Whether the customer has tech support or not (yes, No, No internet service)

• Streaming TV: Whether the customer has streaming TV or not (Yes, No, No internet service)

•StreamingMovies: Whether the customer has streaming movies or not (Yes, No, No internet service)

# Traget variable
• Churn: Whether the customer churn or not (yes or No)*

# 1. Data Preparation - (EDA & Feature Engineering -Data Analytics )
"""

# Commented out IPython magic to ensure Python compatibility.
#EDA
import numpy as np
import pandas as pd

#data visualations
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

!pip install --upgrade scikit-learn

!pip install --upgrade scikit-learn xgboost



telco_base_data=pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')

telco_base_data.head()

telco_base_data.shape

telco_base_data.info()

"""# Knowling the unique values"""

for col in telco_base_data.columns:
    print("column: {} -Unique Values:  {} ".format(col,telco_base_data[col].unique()))

telco_base_data.columns.values

telco_base_data.TotalCharges = pd.to_numeric(telco_base_data.TotalCharges, errors='coerce')

telco_base_data.dtypes

telco_base_data.describe()

"""SeniorCitizen is actually a categorical hence the 25%-50%-75% distribution is not propoer

75% customers have tenure less than 55 months

Average Monthly charges are USD 64.76 whereas 25% customers pay more than USD 89.85 per month
"""

telco_base_data['Churn'].value_counts().plot(kind='barh')
plt.xlabel("Count")
plt.ylabel("Churn")
plt.title("Count of Churn")
plt.gca().invert_yaxis()  # Invert y-axis to have 'No Churn' on top
plt.show()

telco_base_data['Churn'].value_counts()/len(telco_base_data)

telco_base_data['Churn'].value_counts()

telco_base_data.info(verbose=True)

telco_data=telco_base_data.copy()

telco_data.isna().sum()

telco_data.loc[telco_data['TotalCharges'].isna()==True]

telco_data.dtypes

telco_data.isna().sum()/len(telco_data)

"""# 4. Missing Value Treatement

Since the % of these records compared to total dataset is very low ie 0.0015%, it is safe to ignore them from further processing.

"""

#Removing missing values
telco_data.dropna(how = 'any', inplace = True)

"""5. Divide customers into bins based on tenure e.g. for tenure < 12 months: assign a tenure group if 1-12, for tenure between 1 to 2 Yrs, tenure group of 13-24; so on..."""

# Get the max tenure
print(telco_data['tenure'].max()) #72

bins = [0, 12, 24, 36, 48, 60, 72]
labels = ['1 - 12', '13 - 24', '25 - 36', '37 - 48', '49 - 60', '61 - 72']

telco_data['tenure_group'] = pd.cut(telco_data['tenure'], bins=bins, labels=labels, right=True, include_lowest=True)

telco_data['tenure_group'].value_counts()

telco_data['tenure_group'].value_counts()/len(telco_data)

telco_data['tenure_group']

telco_data.isna().sum()

"""6. Remove columns not required for processing"""

#drop column customerID and tenure
telco_data.drop(columns= ['customerID','tenure'], axis=1, inplace=True)
telco_data.head()

"""Data Exploration
*1. * Plot distibution of individual predictors by churn

Univariate Analysis
"""

for i, predictor in enumerate(telco_data.drop(columns=['Churn', 'TotalCharges', 'MonthlyCharges'])):
    plt.figure(i)
    sns.countplot(data=telco_data, x=predictor, hue='Churn')

"""2. Convert the target variable 'Churn' in a binary numeric variable i.e. Yes=1 ; No = 0"""

telco_data['Churn'] = np.where(telco_data.Churn == 'Yes',1,0)

telco_data.sample(3)

telco_data.dtypes

telco_data.loc[telco_data['tenure_group'].isna()==True]

telco_data.isna().sum()

telco_data.isna().sum()

telco_data
telco_data.info() #convert tenure_group into numerical

#telco_data['tenure_group']=pd.to_numeric(telco_data['tenure_group'],errors='coerce')
#telco_base_data.TotalCharges = pd.to_numeric(telco_base_data.TotalCharges, errors='coerce')

telco_data.tenure_group.isna().any()

telco_data.info()

"""3. Convert all the categorical variables into dummy variables"""

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

le

categ=['gender','SeniorCitizen','Partner', 'Dependents', 'PhoneService',
       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
       'Contract', 'PaperlessBilling', 'PaymentMethod',  'Churn','tenure_group']

for i in categ:
    x=telco_data[i].unique()
    print('unique value of',i,'is', x) #knoing the unique value of the each column for encoding purpose

import pandas as pd

# Assuming your DataFrame is df

# Gender
telco_data['gender'] = telco_data['gender'].map({'Female': 0, 'Male': 1})

# SeniorCitizen
telco_data['SeniorCitizen'] = telco_data['SeniorCitizen']

# Partner
telco_data['Partner'] = telco_data['Partner'].map({'No': 0, 'Yes': 1})

# Dependents
telco_data['Dependents'] = telco_data['Dependents'].map({'No': 0, 'Yes': 1})

# PhoneService
telco_data['PhoneService'] = telco_data['PhoneService'].map({'No': 0, 'Yes': 1})

# MultipleLines
telco_data['MultipleLines'] = telco_data['MultipleLines'].map({'No phone service': 0, 'No': 1, 'Yes': 2})

# InternetService
telco_data['InternetService'] = telco_data['InternetService'].map({'No': 0, 'DSL': 1, 'Fiber optic': 2})

# OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies
# These columns have 'No internet service' which we will map to 0
telco_data['OnlineSecurity'] = telco_data['OnlineSecurity'].map({'No internet service': 0, 'No': 1, 'Yes': 2})
telco_data['OnlineBackup'] = telco_data['OnlineBackup'].map({'No internet service': 0, 'No': 1, 'Yes': 2})
telco_data['DeviceProtection'] = telco_data['DeviceProtection'].map({'No internet service': 0, 'No': 1, 'Yes': 2})
telco_data['TechSupport'] = telco_data['TechSupport'].map({'No internet service': 0, 'No': 1, 'Yes': 2})
telco_data['StreamingTV'] = telco_data['StreamingTV'].map({'No internet service': 0, 'No': 1, 'Yes': 2})
telco_data['StreamingMovies'] = telco_data['StreamingMovies'].map({'No internet service': 0, 'No': 1, 'Yes': 2})

# Contract
telco_data['Contract'] = telco_data['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})

# PaperlessBilling
telco_data['PaperlessBilling'] = telco_data['PaperlessBilling'].map({'No': 0, 'Yes': 1})

# PaymentMethod
telco_data['PaymentMethod'] =telco_data['PaymentMethod'].map({
    'Electronic check': 0,
    'Mailed check': 1,
    'Bank transfer (automatic)': 2,
    'Credit card (automatic)': 3
})

# Churn (assuming already encoded as 0 and 1)

# tenure_group
telco_data['tenure_group'] = telco_data['tenure_group'].map({
    '1 - 12': 0,
    '13 - 24': 1,
    '25 - 36': 2,
    '37 - 48': 3,
    '49 - 60': 4,
    '61 - 72': 5
})

telco_data

telco_data.isna().sum()

categ=['gender','SeniorCitizen', 'tenure_group' ,'Partner', 'Dependents', 'PhoneService',
       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
       'Contract', 'PaperlessBilling', 'PaymentMethod',  'Churn',]

#telco_data[categ] = telco_data[categ].apply(le.fit_transform)

telco_data.sample(3)

sns.boxplot(data=telco_data[['TotalCharges', 'MonthlyCharges']])

"""*9. * Relationship between Monthly Charges and Total Charges"""

sns.lmplot(data=telco_data, x='MonthlyCharges', y='TotalCharges', fit_reg=False)

"""*10. * Churn by Monthly Charges and Total Charges"""

# kernel density estimate (KDE) plot.
Mth = sns.kdeplot(telco_data.MonthlyCharges[(telco_data["Churn"] == 0) ],
                color="Red", shade = True)
Mth = sns.kdeplot(telco_data.MonthlyCharges[(telco_data["Churn"] == 1) ],
                ax =Mth, color="Blue", shade= True)
Mth.legend(["No Churn","Churn"],loc='upper right')
Mth.set_ylabel('Density')
Mth.set_xlabel('Monthly Charges')
Mth.set_title('Monthly charges by churn')

"""# Insight: Churn is high when Monthly Charges are high"""

Tot = sns.kdeplot(telco_data.TotalCharges[(telco_data["Churn"] == 0) ],
                color="Red", shade = True)
Tot = sns.kdeplot(telco_data.TotalCharges[(telco_data["Churn"] == 1) ],
                ax =Tot, color="Blue", shade= True)
Tot.legend(["No Churn","Churn"],loc='upper right')
Tot.set_ylabel('Density')
Tot.set_xlabel('Total Charges')
Tot.set_title('Total charges by churn')

"""*11. Build a corelation of all predictors with 'Churn' *"""

plt.figure(figsize=(20,8))
telco_data.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')

"""*Derived Insight: *

HIGH Churn seen in case of Month to month contracts, No online security, No Tech support, First year of subscription and Fibre Optics Internet

LOW Churn is seens in case of Long term contracts, Subscriptions without internet service and The customers engaged for 5+ years

Factors like Gender, Availability of PhoneService and # of multiple lines have alomost NO impact on Churn

This is also evident from the Heatmap below
"""

plt.figure(figsize=(12,12))
sns.heatmap(telco_data.corr(), cmap="Paired")

"""Bivariate Analysis"""

new_df1_target0=telco_data.loc[telco_data["Churn"]==0]
new_df1_target1=telco_data.loc[telco_data["Churn"]==1]

def uniplot(df,col,title,hue =None):

    sns.set_style('whitegrid')
    sns.set_context('talk')
    plt.rcParams["axes.labelsize"] = 20
    plt.rcParams['axes.titlesize'] = 22
    plt.rcParams['axes.titlepad'] = 30


    temp = pd.Series(data = hue)
    fig, ax = plt.subplots()
    width = len(df[col].unique()) + 7 + 4*len(temp.unique())
    fig.set_size_inches(width , 8)
    plt.xticks(rotation=45)
    plt.yscale('log')
    plt.title(title)
    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,palette='bright')

    plt.show()

#uniplot(new_df1_target1,col='Partner',title='Distribution of Gender for Churned Customers',hue='gender')
import seaborn as sns

import seaborn as sns

sns.countplot(x='Partner', hue='gender', data=new_df1_target1)
plt.title('Distribution of Gender for Churned Customers')
plt.tight_layout()
plt.show()

#uniplot(new_df1_target0,col='Partner',title='Distribution of Gender for Non Churned Customers',hue='gender')
sns.countplot(x='Partner', hue='gender', data=new_df1_target0)
plt.title('Distribution of Gender for Churned Customers')
plt.tight_layout()
plt.show()

#uniplot(new_df1_target1,col='PaymentMethod',title='Distribution of PaymentMethod for Churned Customers',hue='gender')
sns.countplot(x='PaymentMethod', hue='gender', data=new_df1_target1)
plt.title('Distribution of Gender for Churned Customers')
plt.tight_layout()
plt.show()

#uniplot(new_df1_target1,col='Contract',title='Distribution of Contract for Churned Customers',hue='gender')
sns.countplot(x='Contract', hue='gender', data=new_df1_target1)
plt.title('Distribution of Gender for Churned Customers')
plt.tight_layout()
plt.show()

#uniplot(new_df1_target1,col='TechSupport',title='Distribution of TechSupport for Churned Customers',hue='gender')
sns.countplot(x='TechSupport', hue='gender', data=new_df1_target1)
plt.title('Distribution of Gender for Churned Customers')
plt.tight_layout()
plt.show()

#uniplot(new_df1_target1,col='SeniorCitizen',title='Distribution of SeniorCitizen for Churned Customers',hue='gender')
sns.countplot(x='SeniorCitizen', hue='gender', data=new_df1_target1)
plt.title('Distribution of Gender for Churned Customers')
plt.tight_layout()
plt.show()

X=telco_data.drop('Churn',axis=1)
y=telco_data['Churn']

X.isna().sum()

X

telco_data['Churn'].value_counts()/len(telco_data) #data is highly imbalancing

"""Train Test Split"""

from sklearn.model_selection  import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

print('Traing data shape')

print(X_train.shape)
print(y_train.shape)




print('Testing Data shape')

print(X_test.shape)
print(y_test.shape)

print(y_test.value_counts())

print(y_train.value_counts())

from sklearn.tree import DecisionTreeClassifier

model_dtc=DecisionTreeClassifier(criterion = "gini",random_state = 100,max_depth=6, min_samples_leaf=8)

model_dtc.fit(X_train,y_train)

model_dtc.score(X_test,y_test)

y_pred=model_dtc.predict(X_test)
y_pred[:10]

print(y_test[:10])

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred, labels=[0,1]))

y.isna().any()

telco_data['tenure_group'].value_counts()#/len(telco_data)

telco_data.isna().sum()

"""''' As you can see that the accuracy is quite low, and as it's an imbalanced dataset, we shouldn't consider Accuracy as our metrics to measure the model, as Accuracy is cursed in imbalanced datasets.
Hence, we need to check recall, precision & f1 score for the minority class, and it's quite evident that the precision, recall & f1 score is too low for Class 1, i.e. churned customers.
Hence, moving ahead to call SMOTEENN (UpSampling + ENN)'''


'''main advantage of using SMOTEENN is that it addresses both overfitting and underfitting issues that can arise from class imbalance. By generating synthetic samples and removing noisy ones'''

"""

from imblearn.over_sampling import SMOTE

smote=SMOTE()

#X_ovs,y_ovs=smote.fit_resample(X,y)

X_ovs, y_ovs = smote.fit_resample(X, y)

fig, oversp = plt.subplots()
oversp.pie( y_ovs.value_counts(), autopct='%.2f')
oversp.set_title("Over-sampling")
plt.show()

X_ovs.isna().sum()
X_ovs.info()

# Importing necessary libraries
import pandas as pd
from sklearn.impute import SimpleImputer

# Assuming X_ovs['tenure_group'] is the column needing imputation
imputer = SimpleImputer(strategy='median')

# Reshape the data to fit the imputer if necessary (depends on data structure)
X_ovs['tenure_group'] = imputer.fit_transform(X_ovs[['tenure_group']])

# If necessary, convert the result back to a DataFrame
X_ovs['tenure_group'] = pd.DataFrame(X_ovs['tenure_group'])

# Verify that missing values have been filled
X_ovs['tenure_group'].isnull().sum()  # Should return 0 if successful

X_ovs['tenure_group'].median()

telco_data['tenure_group'].value_counts()

Xr_train,Xr_test,yr_train,yr_test=train_test_split(X_ovs, y_ovs,test_size=0.2,random_state=42)

from sklearn.linear_model  import LogisticRegression

model_lr=LogisticRegression(max_iter=100)

model_lr.fit(Xr_train,yr_train)

y_pred=model_lr.predict(Xr_test)
y_pred[:10]

model_lr.score(Xr_test,yr_test)

from sklearn.metrics import accuracy_score, classification_report


report = classification_report(y_pred, yr_test, labels=[0, 1])

print(report)

from sklearn.metrics import confusion_matrix
confusion_matrix(yr_test,y_pred)

"""Decision Tree classifier"""

from sklearn.tree import DecisionTreeClassifier

model_dtc=DecisionTreeClassifier(criterion = "gini",random_state = 100,max_depth=6, min_samples_leaf=8)

model_dtc.fit(Xr_train,yr_train)

y_pred=model_dtc.predict(Xr_test)
y_pred[:10]

yr_test[:10]

model_dtc.score(Xr_test,yr_test)

print(classification_report(yr_test, y_pred, labels=[0,1]))

confusion_matrix(yr_test,y_pred)

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

model_rfc=RandomForestClassifier(n_estimators=100, random_state = 100,max_depth=6, min_samples_leaf=8,class_weight='balanced')

Xr_train.columns

model_rfc.fit(Xr_train,yr_train)

y_pred=model_rfc.predict(Xr_test)
y_pred[:10]

yr_test[:10]

model_rfc.score(Xr_test,yr_test)

report_rfc=classification_report(y_pred,yr_test)
print(report_rfc)

confusion_matrix(yr_test,y_pred)

"""AdaBoost"""

from sklearn.ensemble import  AdaBoostClassifier

model_abc=AdaBoostClassifier(n_estimators=100)

model_abc.fit(Xr_train,yr_train)

y_pred=model_abc.predict(Xr_test)

print(classification_report(y_pred,yr_test))

confusion_matrix(yr_test,y_pred)

"""GradientBoostingClassifer"""

from sklearn.ensemble import GradientBoostingClassifier
model_gbc=GradientBoostingClassifier()
model_gbc

model_gbc.fit(Xr_train,yr_train)

y_pred_gbc=model_gbc.predict(Xr_test)
y_pred_gbc[:10]

yr_test[:10]

print(classification_report(y_pred_gbc,yr_test))

confusion_matrix(yr_test,y_pred)

"""Xgboost"""

!pip install xgboost

from xgboost import XGBClassifier

model_xgb=XGBClassifier(class_weight={0:1, 1:2})

from xgboost import XGBClassifier

# Calculate scale_pos_weight
scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])

# Initialize XGBClassifier
model = XGBClassifier(scale_pos_weight=scale_pos_weight)

model_xgb.fit(Xr_train,yr_train)

y_pred=model_xgb.predict(Xr_test)
y_pred[:10]

yr_test[:10]

print(classification_report(y_pred,yr_test))

from sklearn.metrics import confusion_matrix


cm = confusion_matrix(yr_test, y_pred)

print("Confusion Matrix:")
print(cm)

"""finding the best hyperparameter fro XGBclassifier"""

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
import time

# Define your GradientBoostingClassifier and param_dist
model = XGBClassifier()
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of boosting rounds
    'learning_rate': [0.01, 0.1, 0.5],  # Step size shrinkage used in update
    'max_depth': [3, 5, 7],  # Maximum depth of a tree
    'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight needed in a child
    'gamma': [0, 0.1, 0.2],  # Minimum loss reduction required to make a further partition
    'subsample': [0.8, 1.0],  # Subsample ratio of the training instance
    'colsample_bytree': [0.8, 1.0]  # Subsample ratio of columns when constructing each tree
}
# Create RandomizedSearchCV object with fewer iterations
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=5, cv=10, scoring='accuracy', random_state=42)

# Start the timer
start_time = time.time()

# Fit the RandomizedSearchCV object
random_search.fit(Xr_train, yr_train)

# Stop the timer
end_time = time.time()

# Calculate the total time taken
total_time = end_time - start_time

print("RandomizedSearchCV took {:.2f} seconds to complete.".format(total_time))

# Get the best parameters
best_params = random_search.best_params_
print("Best Parameters:", best_params)

"""# final model"""

from xgboost import XGBClassifier

# Define the best hyperparameters obtained from GridSearchCV
best_params = {
   'subsample': 0.8, 'n_estimators': 200, 'min_child_weight': 5, 'max_depth': 7, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.8


}

# Create Gradient Boosting Classifier with the best hyperparameters
final_XGB_classifier = XGBClassifier(**best_params)

# Train the final model on the entire training data
final_XGB_classifier.fit(Xr_train, yr_train)

from sklearn.model_selection import cross_val_score

# trained model with tuned hyperparameters
# X_train and y_train are your training data
# cv=10 indicates 10-fold cross-validation
cv_scores = cross_val_score(final_XGB_classifier, Xr_train, yr_train, cv=10, scoring='accuracy')

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Mean CV score:", cv_scores.mean())

y_pred=final_XGB_classifier.predict(Xr_test)
y_pred[:10]

yr_test[:10]

print(classification_report(y_pred,yr_test))

confusion_matrix(y_pred,yr_test)

"""Electronic check medium are the highest churners

Contract Type - Monthly customers are more likely to churn because of no contract terms, as they are free to go customers.

No Online security, No Tech Support category are high churners

Non senior Citizens are high churners

Pickle file
"""

from xgboost import XGBClassifier
import joblib

# Assuming grid_search contains the best trained model from GridSearchCV
# Define the XGBoost classifier with best parameters
final_XGB_classifier = XGBClassifier(
    objective='binary:logistic',
    subsample=0.8,
    n_estimators=200,
    min_child_weight=5,
    max_depth=7,
    learning_rate=0.1,
    gamma=0,
    colsample_bytree=0.8,
    random_state=42  # Ensure reproducibility
)
# Fit the classifier on your data (assuming X_train, y_train are defined)
final_XGB_classifier.fit(Xr_train, yr_train)

# Save the model to a file
joblib.dump(final_XGB_classifier, 'final_xgb_model.pkl')


# Load the model from the file
loaded_model = joblib.load('final_xgb_model.pkl')

# Use the loaded model to make predictions
y_pred = loaded_model.predict(Xr_test)

y_pred

"""Checking accuravy with our features

"""

import pickle
import pandas as pd

# Load the saved model from the pickle file
with open('final_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Prepare your own data for testing
# Create a DataFrame with your feature data
your_features = pd.DataFrame({
    'gender': [1, 0, 0, 0, 0],
    'SeniorCitizen': [0, 0, 0, 0, 0],
    'Partner': [0, 0, 0, 1, 1],
    'Dependents': [0, 0, 0, 0, 1],
    'PhoneService': [1, 0, 1, 1, 1],
    'MultipleLines': [0, 0, 0, 2, 2],
    'InternetService': [1, 0, 1, 1, 0],
    'OnlineSecurity': [0, 0, 0, 2, 2],
    'OnlineBackup': [0, 0, 1, 2, 2],
    'DeviceProtection': [0, 0, 0, 0, 2],
    'TechSupport': [0, 0, 0, 2, 2],
    'StreamingTV': [0, 1, 0, 0, 0],
    'StreamingMovies': [0, 1, 0, 0, 0],
    'Contract': [2, 0, 0, 1, 2],
    'PaperlessBilling': [0, 1, 0, 0, 0],
    'PaymentMethod': [1, 1, 1, 0, 0],
    'MonthlyCharges': [90.407734, 58.273891, 74.379767, 108.55, 64.35],
    'TotalCharges': [707.535237, 3264.466697, 1146.937795, 5610.7, 1558.65],
    'tenure_group': [0, 4, 1, 4, 2]
})

# Make predictions using the loaded model on your own data
predictions = loaded_model.predict(your_features)

# Print the predictions
print("Predictions:", predictions)

import pickle
import pandas as pd

# Load the saved model from the pickle file
with open('final_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Prepare your own data for testing
# Create a DataFrame with your feature data
your_features = pd.DataFrame({
    'gender': [1, 0, 0, 0, 0],
    'SeniorCitizen': [0, 0, 0, 0, 0],
    'Partner': [0, 0, 0, 1, 1],
    'Dependents': [0, 0, 0, 0, 1],
    'PhoneService': [1, 0, 1, 1, 1],
    'MultipleLines': [0, 0, 0, 2, 2],
    'InternetService': [1, 0, 1, 1, 0],
    'OnlineSecurity': [0, 0, 0, 2, 2],
    'OnlineBackup': [0, 0, 1, 2, 2],
    'DeviceProtection': [0, 0, 0, 0, 2],
    'TechSupport': [0, 0, 0, 2, 2],
    'StreamingTV': [0, 1, 0, 0, 0],
    'StreamingMovies': [0, 1, 0, 0, 0],
    'Contract': [2, 0, 0, 1, 2],
    'PaperlessBilling': [0, 1, 0, 0, 0],
    'PaymentMethod': [1, 1, 1, 0, 0],
    'MonthlyCharges': [90.407734, 58.273891, 74.379767, 108.55, 64.35],
    'TotalCharges': [707.535237, 3264.466697, 1146.937795, 5610.7, 1558.65],
    'tenure_group': [0, 4, 1, 4, 2]
})

# Make predictions using the loaded model on your own data
predictions = loaded_model.predict(your_features)

# Map predictions to human-readable labels
predicted_labels = ['Not Churn' if pred == 0 else 'Churn' for pred in predictions]

# Create a DataFrame with predictions
predictions_df = pd.DataFrame({
    'Prediction': predicted_labels
})

# Combine original data with predictions
output_df = pd.concat([your_features, predictions_df], axis=1)

# Print the output
print(output_df)

import pickle
import pandas as pd

# Load the saved model from the pickle file
with open('final_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Prepare your own data for testing
# Create a DataFrame with your feature data

your_features = pd.DataFrame({
    'gender': [1, 0, 0, 0, 0],  # 1 for Male, 0 for Female
    'SeniorCitizen': [0, 0, 0, 0, 0],  # 1 for Senior Citizen, 0 for Not Senior Citizen
    'Partner': [0, 0, 0, 1, 1],  # 1 for Yes, 0 for No (whether the customer has a partner)
    'Dependents': [0, 0, 0, 0, 1],  # 1 for Yes, 0 for No (whether the customer has dependents)
    'PhoneService': [1, 0, 1, 1, 1],  # 1 for Yes, 0 for No (whether the customer has phone service)
    'MultipleLines': [0, 0, 0, 2, 2],  # 0 for No phone service, 1 for No multiple lines, 2 for Multiple lines
    'InternetService': [1, 0, 1, 1, 0],  # 0 for No internet service, 1 for DSL, 2 for Fiber optic
    'OnlineSecurity': [0, 0, 0, 2, 2],  # 0 for No internet service, 1 for No online security, 2 for Yes online security
    'OnlineBackup': [0, 0, 1, 2, 2],  # 0 for No internet service, 1 for No online backup, 2 for Yes online backup
    'DeviceProtection': [0, 0, 0, 0, 2],  # 0 for No internet service, 1 for No device protection, 2 for Yes device protection
    'TechSupport': [0, 0, 0, 2, 2],  # 0 for No internet service, 1 for No tech support, 2 for Yes tech support
    'StreamingTV': [0, 1, 0, 0, 0],  # 0 for No internet service, 1 for No streaming TV, 2 for Yes streaming TV
    'StreamingMovies': [0, 1, 0, 0, 0],  # 0 for No internet service, 1 for No streaming movies, 2 for Yes streaming movies
    'Contract': [2, 0, 0, 1, 2],  # 0 for Month-to-month, 1 for One year, 2 for Two year
    'PaperlessBilling': [0, 1, 0, 0, 0],  # 1 for Yes, 0 for No (whether the customer uses paperless billing)
    'PaymentMethod': [1, 1, 1, 0, 0],  # 0 for Electronic check, 1 for Mailed check, 2 for Bank transfer (automatic), 3 for Credit card (automatic)
    'MonthlyCharges': [90.407734, 58.273891, 74.379767, 108.55, 64.35],  # Monthly charges
    'TotalCharges': [707.535237, 3264.466697, 1146.937795, 5610.7, 1558.65],  # Total charges over customer tenure
    'tenure_group': [0, 4, 1, 4, 2]  # Tenure group categories
})

# Make predictions using the loaded model on your own data
predictions = loaded_model.predict(your_features)

# Map predictions to human-readable labels
predicted_labels = ['Not Churn' if pred == 0 else 'Churn' for pred in predictions]

# Create a DataFrame with predictions
predictions_df = pd.DataFrame({
    'Prediction': predicted_labels
})

# Combine original data with predictions
output_df = pd.concat([your_features, predictions_df], axis=1)

# Print the output
print(output_df)

import pickle
import pandas as pd

# Load the saved model from the pickle file
with open('final_xgb_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Prepare your own data for testing
# Create a DataFrame with your feature data

your_features = pd.DataFrame({
    'gender': [1],  # Selecting the first value for gender
    'SeniorCitizen': [0],  # Selecting the first value for SeniorCitizen
    'Partner': [0],  # Selecting the first value for Partner
    'Dependents': [0],  # Selecting the first value for Dependents
    'PhoneService': [1],  # Selecting the first value for PhoneService
    'MultipleLines': [0],  # Selecting the first value for MultipleLines
    'InternetService': [1],  # Selecting the first value for InternetService
    'OnlineSecurity': [0],  # Selecting the first value for OnlineSecurity
    'OnlineBackup': [0],  # Selecting the first value for OnlineBackup
    'DeviceProtection': [0],  # Selecting the first value for DeviceProtection
    'TechSupport': [0],  # Selecting the first value for TechSupport
    'StreamingTV': [0],  # Selecting the first value for StreamingTV
    'StreamingMovies': [0],  # Selecting the first value for StreamingMovies
    'Contract': [2],  # Selecting the first value for Contract
    'PaperlessBilling': [0],  # Selecting the first value for PaperlessBilling
    'PaymentMethod': [1],  # Selecting the first value for PaymentMethod
    'MonthlyCharges': [70.407734],  # Selecting the first value for MonthlyCharges
    'TotalCharges': [707.535237],  # Selecting the first value for TotalCharges
    'tenure_group': [4]  # Selecting the first value for tenure_group
})

# Make predictions using the loaded model on your own data
predictions = loaded_model.predict(your_features)

# Map predictions to human-readable labels
predicted_labels = ['Not Churn' if pred == 0 else 'Churn' for pred in predictions]

# Create a DataFrame with predictions
predictions_df = pd.DataFrame({
    'Prediction': predicted_labels
})

# Combine original data with predictions
output_df = pd.concat([your_features, predictions_df], axis=1)

# Print the output
print(output_df)

